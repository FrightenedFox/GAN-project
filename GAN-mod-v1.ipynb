{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e598af-53e0-4ba4-8a4f-d493d6b81bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a6baa3e3-b91c-40cb-a7a0-e0020f98a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2    # number of epochs of training\n",
    "BATCH_SIZE = 50   # size of the batches\n",
    "LR = 1e-5         # adam: learning rate\n",
    "B1 = 0.5          # adam: decay of first order momentum of gradient\n",
    "B2 = 0.999        # adam: decay of first order momentum of gradient\n",
    "\n",
    "N_CPU = 11         # number of cpu threads to use during batch generation\n",
    "LATENT_DIM = 100  # dimensionality of the latent space\n",
    "IMG_SIZE = 28     # size of each image dimension\n",
    "CHANNELS = 1      # number of image channels\n",
    "SAMPLE_INTERVAL = 100 # interval betwen image samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b382d675-d0ec-4717-8120-cba1ec9501e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n"
     ]
    }
   ],
   "source": [
    "img_shape = (CHANNELS, IMG_SIZE, IMG_SIZE)\n",
    "torch.manual_seed(12345)\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(\"CUDA:\", cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0272a859-eb04-46a5-83a9-96cf98917a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 7, 7, 4, 4])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Unconv2d(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_channels, kernel_size, input_size, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_channels = kernel_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(1, in_channels * kernel_channels, *input_size, *kernel_size),\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.Tensor(1, in_channels * kernel_channels, *input_size, *kernel_size),\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)   \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "            \n",
    "    def extra_repr(self):\n",
    "        return \"in_channels={}, kernel_channels={}, bias={}\".format(\n",
    "            self.in_channels, self.kernel_channels, self.bias is not None\n",
    "        ) + \"kernel_size={}, input_size={}\".format(self.kernel_size, self.input_size) \n",
    "        \n",
    "    def forward(self, input_):\n",
    "        input_ = input_.repeat(1, self.kernel_channels, 1, 1, 1, 1)\n",
    "        unbiased = torch.mul(self.weight, input_)\n",
    "        if self.bias is None:\n",
    "            return unbiased\n",
    "        output = torch.add(self.bias, unbiased)\n",
    "        return output\n",
    "    \n",
    "unc = Unconv2d(10, 20, (4, 4), (7, 7), bias=False)\n",
    "t = torch.randn(10, 7, 7, 1, 1)\n",
    "unc(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5bdf4-5b04-47f0-9cec-2188d502125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedImage(nn.Module):\n",
    "    def __init__(self, in_channels, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(1, in_channels, 1, 1),\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.Tensor(1, in_channels, 1, 1),\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)   \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "            \n",
    "    def extra_repr(self):\n",
    "        return \"in_channels={}, bias={}\".format(\n",
    "            self.in_channels, self.bias is not None\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        unbiased = torch.mul(self.weight, input_)\n",
    "        if self.bias is None:\n",
    "            return unbiased\n",
    "        output = torch.add(self.bias, unbiased)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d43e4edf-7bf5-4df0-83d3-772917567cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inner_fc_size=128,\n",
    "                 small_imag_size=(7, 7),\n",
    "                 small_channels=64,\n",
    "                 big_imag_kernel_size=(4, 4),\n",
    "                 big_imag_kernel_channels=2                 \n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.inner_fc_size = inner_fc_size\n",
    "        self.small_imag_size = small_imag_size\n",
    "        self.small_channels = small_channels\n",
    "        self.big_imag_kernel_size = big_imag_kernel_size\n",
    "        self.big_imag_kernel_channels = big_imag_kernel_channels        \n",
    "        \n",
    "        self.fc1 = nn.Linear(LATENT_DIM, inner_fc_size)\n",
    "        self.fc2 = nn.Linear(\n",
    "            inner_fc_size, \n",
    "            small_imag_size[0] * small_imag_size[1] * small_channels\n",
    "        )\n",
    "        self.unconv = Unconv2d(\n",
    "            small_channels, big_imag_kernel_channels, big_imag_kernel_size, small_imag_size\n",
    "        )\n",
    "        self.weim = WeightedImage(small_channels * big_imag_kernel_channels)\n",
    "        \n",
    "    \n",
    "    def tt(self, m, n, i, j):\n",
    "        t1 = torch.arange(0, m * n, i * j).view(int(m / i), int(n / j)).repeat_interleave(i, dim=0).repeat_interleave(j, dim=1)\n",
    "        t2 = torch.arange(i * j).view(i, j).repeat(int(m / i), int(n / j))\n",
    "        return (t1 + t2).view(m * n)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        flat_z = F.leaky_relu(self.fc1(z), 0.2)\n",
    "        flat_z = F.leaky_relu(self.fc2(flat_z), 0.2)\n",
    "        \n",
    "        small_imgs = flat_z.view(\n",
    "            BATCH_SIZE, \n",
    "            self.small_channels, \n",
    "            *self.small_imag_size, \n",
    "            1, 1\n",
    "        )\n",
    "        \n",
    "        big_imgs = self.unconv(small_imgs)  \n",
    "        big_imgs = big_imgs.view(\n",
    "            BATCH_SIZE, \n",
    "            self.small_channels * self.big_imag_kernel_channels, \n",
    "            IMG_SIZE * IMG_SIZE\n",
    "        ).transpose(0, -1)\n",
    "        big_imgs = big_imgs[\n",
    "            self.tt(IMG_SIZE, IMG_SIZE, *self.big_imag_kernel_size)\n",
    "        ].transpose(0, -1)\n",
    "        big_imgs = F.leaky_relu(big_imgs.view(\n",
    "            BATCH_SIZE, \n",
    "            self.small_channels * self.big_imag_kernel_channels, \n",
    "            IMG_SIZE, \n",
    "            IMG_SIZE\n",
    "        ), 0.2)\n",
    "        \n",
    "        weighted_img = self.weim(big_imgs)\n",
    "#         weighted_img = F.softmax(torch.sum(weighted_img, dim=1))\n",
    "        weighted_img = torch.tanh(torch.sum(weighted_img, dim=1))\n",
    "    \n",
    "        return weighted_img.view(weighted_img.size(0), *img_shape)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "42418403-6cb9-494a-9548-cfb3115b1a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "#         print(\"validity:\\n\", validity)\n",
    "#         print(validity.shape)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ca719bfb-8a77-4b8e-8cad-7c90a7034abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f26c3a47-2160-41e4-90a1-d92f604a3938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "# os.makedirs(\"../PyTorch/MNIST\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(IMG_SIZE), \n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize([0.5], [0.5])\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "992f65c0-ec9d-4f6f-a0e5-fc288f477797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR, betas=(B1, B2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR, betas=(B1, B2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "20796dc5-15ce-47c3-adb4-ec676f7979eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_imgs:  torch.Size([50, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = Variable(Tensor(np.random.normal(0, 1, (BATCH_SIZE, LATENT_DIM))))\n",
    "\n",
    "generator = Generator()\n",
    "gen_imgs = generator(z)\n",
    "plt.imshow(gen_imgs[0][0].detach().numpy())\n",
    "print(\"gen_imgs: \", gen_imgs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ab86a4f7-459b-4063-b56d-4c123907712f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0/2400] [D loss: 0.680889] [G loss: 0.692658]\n",
      "[Batch 100/2400] [D loss: 0.482982] [G loss: 0.655844]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-e2f5e8f61c30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mg_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\PyTorch-Jupyter\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\PyTorch-Jupyter\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], LATENT_DIM))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % SAMPLE_INTERVAL == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "            print(\n",
    "                \"[Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (batches_done, len(dataloader)*N_EPOCHS, d_loss.item(), g_loss.item())\n",
    "            ) \n",
    "    print(\n",
    "            \"--- [Epoch %d/%d] [D loss: %f] [G loss: %f] ---\"\n",
    "            % (epoch, N_EPOCHS, d_loss.item(), g_loss.item())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685122d-3f2b-48cb-b652-ab6d4c167ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(gen_imgs[0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ff713-b648-430d-ab30-ed4d5366e452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
